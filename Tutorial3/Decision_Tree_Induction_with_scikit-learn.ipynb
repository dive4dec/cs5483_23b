{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa8ed30",
   "metadata": {},
   "source": [
    "---\n",
    "title: Decision Tree Induction with scikit-learn\n",
    "math: \n",
    "    '\\abs': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\norm': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "    '\\mc': '\\mathcal{#1}'\n",
    "    '\\M': '\\boldsymbol{#1}'\n",
    "    '\\R': '\\mathsf{#1}'\n",
    "    '\\RM': '\\boldsymbol{\\mathsf{#1}}'\n",
    "    '\\op': '\\operatorname{#1}'\n",
    "    '\\E': '\\op{E}'\n",
    "    '\\d': '\\mathrm{\\mathstrut d}'\n",
    "    '\\Gini': '\\operatorname{Gini}'\n",
    "    '\\Info': '\\operatorname{Info}'\n",
    "    '\\Gain': '\\operatorname{Gain}'\n",
    "    '\\GainRatio': '\\operatorname{GainRatio}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec40cb8e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f6866",
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, tree\n",
    "\n",
    "%matplotlib widget\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38685f",
   "metadata": {},
   "source": [
    "## Decision Tree Induction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e490fbe2",
   "metadata": {},
   "source": [
    "We will use the [*iris dataset*](https://en.wikipedia.org/wiki/Iris_flower_data_set) from [`sklearn.datasets` package](https://scikit-learn.org/stable/datasets/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19230e",
   "metadata": {},
   "source": [
    "Recall that the classification task is to train a model that can automatically classify the species (*target*) based on the lengths and widths of the petals and sepals (*input features*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c3004",
   "metadata": {},
   "source": [
    "To build a decision tree, we use `DecisionTreeClassifier` from `sklearn.tree` and apply its `fit` method on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = tree.DecisionTreeClassifier(random_state=0).fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33e538",
   "metadata": {},
   "source": [
    "To display the decision tree, we can use the function `plot_tree` from `sklearn.tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "tree.plot_tree(clf_gini)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b18ae3",
   "metadata": {},
   "source": [
    "To make the decision tree looks better, we can provide additional options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a448b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"feature_names\": iris.feature_names,\n",
    "    \"class_names\": iris.target_names,\n",
    "    \"label\": \"root\",\n",
    "    \"filled\": True,\n",
    "    \"node_ids\": True,\n",
    "    \"proportion\": True,\n",
    "    \"rounded\": True,\n",
    "    \"fontsize\": 7,\n",
    "}  # store options as dict for reuse\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "tree.plot_tree(clf_gini, **options)  # ** unpacks dict as keyword arguments\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc31c6",
   "metadata": {},
   "source": [
    "In particular, the iris setosa is distinguished immediately after checking the petal width/length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f291bf",
   "metadata": {},
   "source": [
    "::::{note}\n",
    "\n",
    "For each node:\n",
    "- `___ <= ___` is the splitting criterion for internal nodes, satisfied only by samples going left.\n",
    "- `gini = ...` shows the impurity index. By default, the algorithm uses the Gini impurity index to find the best binary split. Observe that the index decreases down the tree towards the leaves.\n",
    "- `value = [_, _, _]` shows the number/fraction of examples for each of the three classes, and `class = ...` indicates the majority class, which may be used as the decision for a leaf node. The majority classes are also color coded. Observe that the color gets lighter towards the root as the class distribution becomes more impure. \n",
    "\n",
    "The information of the decision is stored in the `tree_` attribute of the classifier. For more details, run `help(clf_gini.tree_)`.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee807a66",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:clf_entropy\n",
    "\n",
    "Assign to `clf_entropy` the decision tree classifier created using *entropy* as the impurity measure. You can do so with the keyword argument `criterion='entropy'` in `DecisionTreeClassifier`. Furthermore, Use `random_state=0` and fit the classifier on the entire iris dataset. Check whether the resulting decision tree is the same as the one created using the Gini impurity index.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4779a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b5af6f05d8ce6f84d7a2178a5ed5a53",
     "grade": false,
     "grade_id": "tree-entropy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "tree.plot_tree(clf_entropy, **options)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160ce88",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b33243c1128464d72060ac89b3646dd1",
     "grade": true,
     "grade_id": "same-tree-as-gini",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81880853",
   "metadata": {},
   "source": [
    "::::{caution}\n",
    "\n",
    "Although one can specify whether to use Gini impurity or entropy, `sklearn` implements neither C4.5 nor CART. In particular, it supports only binary splits on numeric input attributes, unlike C4.5, which supports multi-way splits using the information gain ratio. See a workaround [here][categorical].\n",
    "\n",
    "[categorical]: https://stackoverflow.com/questions/38108832/passing-categorical-data-to-sklearn-decision-tree\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2337a96",
   "metadata": {},
   "source": [
    "## Splitting Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27f8e8",
   "metadata": {},
   "source": [
    "To induce a good decision tree efficiently, the splitting criterion is chosen \n",
    "- greedily to maximize the reduction in impurity and \n",
    "- recursively starting from the root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad079454",
   "metadata": {},
   "source": [
    "### Overview using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206bf97a",
   "metadata": {},
   "source": [
    "To have a rough idea of what are good features to split on, we will use [pandas](https://pandas.pydata.org/docs/user_guide/index.html) [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas.DataFrame) \n",
    "to operate on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98969448",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9gQINfrjsb4M",
    "outputId": "77b77a38-2712-4c93-c503-219e74f354fd",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# write the input features first\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# append the target values to the last column\n",
    "df[\"target\"] = iris.target\n",
    "df.target = df.target.astype(\"category\").cat.rename_categories(\n",
    "    dict(zip(range(3), iris.target_names))\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a4820",
   "metadata": {},
   "source": [
    "To display some statistics of the input features for different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"target\", observed=False).boxplot(rot=90, layout=(1, 3), figsize=(7, 9))\n",
    "df.groupby(\"target\", observed=False).agg([\"mean\", \"std\"]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5916ce",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:good_features\n",
    "\n",
    "Identify good feature(s) based on the above statistics. Does your choice agree with the decision tree generated by `DecisionTreeClassifier`?\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa128e66",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23015178e587e6f2ad5fd85735da2221",
     "grade": true,
     "grade_id": "good-features",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70537564",
   "metadata": {},
   "source": [
    "### Measuring impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e1815",
   "metadata": {},
   "source": [
    "Suppose nearly all instances of a dataset belong to the same class. In that case, we can return the majority class as the decision without further splitting. A measure of impurity is the Gini impurity index, defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba846ed",
   "metadata": {},
   "source": [
    "::::{prf:definition}\n",
    "\n",
    "Given a dataset $D$ with a class attribute (discrete target), the Gini impurity index is defined as\n",
    "\n",
    "$$\n",
    "\\Gini(D):= g(p_0,p_1,\\dots)\n",
    "$$ (Gini)\n",
    "\n",
    "where $(p_0,p_1,\\dots)$ are probability masses corresponding to the empirical class distribution of $D$, and\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(p_0,p_1,\\dots) &:= \\sum_k p_k(1-p_k)\\\\\n",
    "&= 1- \\sum_k p_k^2.\n",
    "\\end{align}\n",
    "$$ (g)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f6a35",
   "metadata": {},
   "source": [
    "::::{note}\n",
    "\n",
    "For convenience, we may also write\n",
    "\n",
    "- $g(\\boldsymbol{p})$ for the stochastic vector $\\boldsymbol{p}=\\begin{bmatrix}p_0 & p_1 & \\dots\\end{bmatrix}$ of probability masses, and\n",
    "- $g(p)$ for the probability mass function $p: k \\mapsto p_k$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a857cf",
   "metadata": {},
   "source": [
    "We can represent a distribution simply as a numpy array. To return the empirical class distributions of the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb89de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(values):\n",
    "    \"\"\"Returns the empirical distribution of the given 1D array of values as a\n",
    "    1D array of probabilities. (The ordering is immaterial.)\"\"\"\n",
    "    counts = np.unique(values, return_counts=True)[-1]\n",
    "    return counts / counts.sum()\n",
    "\n",
    "\n",
    "print(f\"Distribution of target: {dist(iris.target).round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5be6b9",
   "metadata": {},
   "source": [
    "The Gini impurity index can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0290d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(p):\n",
    "    \"\"\"Returns the Gini impurity of the distribution p.\"\"\"\n",
    "    return 1 - (p**2).sum()\n",
    "\n",
    "\n",
    "print(f\"Gini(D) = {g(dist(iris.target)):.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6cc31",
   "metadata": {},
   "source": [
    "Another measure of impurity uses the information measure called entropy in information theory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2dc83b",
   "metadata": {},
   "source": [
    "::::{prf:definition}\n",
    "\n",
    "The information content is defined as \n",
    "\n",
    "$$\n",
    "\\Info(D):= h(p_0,p_1,\\dots),\n",
    "$$ (Info)\n",
    "\n",
    "which is the entropy of the class distribution\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h(\\boldsymbol{p}) = h(p_0,p_1,\\dots) &= \\sum_{k:p_k>0} p_k \\log_2 \\frac1{p_k}.\n",
    "\\end{align}\n",
    "$$ (h)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1689b4",
   "metadata": {},
   "source": [
    "::::{note}\n",
    "\n",
    "- For convenience, we often omit the constraint $p_k>0$ by regarding $0 \\log \\frac10$ as the limit $\\lim_{p\\to 0} p \\log \\frac1{p} = 0$.\n",
    "- Unless otherwise specified, all the logarithm is base 2, i.e., $\\log = \\lg$, where the information quantities are in the unit of bit (binary digit). A popular alternative is to use the natural logarithm, $\\log = \\ln$, where the unit is in *nat*.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e569373",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:h\n",
    "\n",
    "Complete the following function to compute the entropy of a distribution. You may use the function `log2` from `numpy` to calculate the logarithm base 2.\n",
    "\n",
    ":::{hint}\n",
    ":class: dropdown\n",
    "\n",
    "Consider the solution template:\n",
    "\n",
    "```python\n",
    "def h(p):\n",
    "    ...\n",
    "    return (p * ___ * ___).sum()\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb12da",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5685869188b402f81872425f018a8b9",
     "grade": false,
     "grade_id": "entropy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def h(p):\n",
    "    \"\"\"Returns the entropy of distribution p (1D array).\"\"\"\n",
    "    p = np.array(p)\n",
    "    p = p[(p > 0) & (p < 1)]  # 0 log 0 = 1 log 1 = 0\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "print(f\"Info(D): {h(dist(iris.target)):.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057f811",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10416def596cc71da7effe5c3946e7a1",
     "grade": true,
     "grade_id": "test-entropy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert np.isclose(h([1 / 2, 1 / 2]), 1)\n",
    "assert np.isclose(h([1, 0]), 0)\n",
    "assert np.isclose(h([1 / 2, 1 / 4, 1 / 4]), 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92c675",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75c7dda125858a45852837a19a595057",
     "grade": true,
     "grade_id": "htest-entropy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42ab88",
   "metadata": {},
   "source": [
    "### Drop in impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2fd8e",
   "metadata": {},
   "source": [
    "::::{prf:definition} \n",
    "\n",
    "The drop in Gini impurity for a splitting criterion $A$ on a dataset $D$ with respect to the class attribute is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\Gini_{\\R{A}}(D) &:= \\Gini(D) - \\Gini_{\\R{A}}(D)\\\\\n",
    "\\Gini_{\\R{A}}(D) &:= \\sum_{j} \\frac{|D_j|}{|D|} \\Gini(D_j),\n",
    "\\end{align}\n",
    "$$ (Delta-Gini)\n",
    "\n",
    "where $D$ is split by $A$ into $D_j$ for different outcomes $j$ of the split.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a30bd3",
   "metadata": {},
   "source": [
    "We will consider the binary splitting criterion $\\R{X}\\leq s$ in particular, which gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\Gini_{\\R{A}}(D) = g(\\hat{P}_\\R{Y}) - \\left[\\hat{P}[\\R{X}\\leq s] g(\\hat{p}_{\\R{Y}|\\R{X}\\leq s}) + \\hat{P}[\\R{X}> s]g(\\hat{p}_{\\R{Y}|\\R{X}> s})\\right]\n",
    "\\end{align}\n",
    "$$ (Delta-Gini-binary)\n",
    "\n",
    "where \n",
    "\n",
    "- $\\R{Y}$ denotes the target,\n",
    "- $\\hat{P}$ denotes the empirical distribution, and\n",
    "- $\\hat{p}_{\\R{Y}|\\R{X}\\leq s}$, $\\hat{p}_{\\R{Y}|\\R{X}> s}$, and $\\hat{p}_{\\R{Y}}$ denote the empirical probability mass functions of $\\R{Y}$ with or without conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_in_gini(X, Y, s):\n",
    "    \"\"\"Returns the drop in Gini impurity of the target Y\n",
    "    for the binary splitting criterion X <= s.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: 1D array\n",
    "        Input feature values for different instances.\n",
    "    Y: 1D array\n",
    "        Target values corresponding to X.\n",
    "    s: Splitting point for X.\n",
    "    \"\"\"\n",
    "    S = X <= s\n",
    "    q = S.mean()\n",
    "    return g(dist(Y)) - q * g(dist(Y[S])) - (1 - q) * g(dist(Y[~S]))\n",
    "\n",
    "\n",
    "X, Y = df[\"petal width (cm)\"], df.target\n",
    "print(f\"Drop in Gini: {drop_in_gini(X, Y, 0.8):.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6326c658",
   "metadata": {},
   "source": [
    "To compute the best splitting point for a given input feature, we check every consecutive mid-points of the observed feature values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split_pt(X, Y, gain_function):\n",
    "    \"\"\"Return the best splitting points and the maximum gain evaluated using\n",
    "    gain_function for the split X <= s and target Y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: 1D array\n",
    "        Input feature values for different instances.\n",
    "    Y: 1D array\n",
    "        Target values corresponding to X.\n",
    "    gain_function: function of (X, Y, x)\n",
    "        A function such as drop_in_gini for evaluating a\n",
    "        splitting criterion X <= s.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (s, g) where s is the best splitting point and g is the maximum gain.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    drop_in_gini\n",
    "    \"\"\"\n",
    "    values = np.sort(np.unique(X))\n",
    "    split_pts = (values[1:] + values[:-1]) / 2\n",
    "    gain = np.array([gain_function(X, Y, s) for s in split_pts])\n",
    "    i = np.argmax(gain)\n",
    "    return split_pts[i], gain[i]\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\"\"Best split point: {0:.3g}\n",
    "Maximum gain: {1:.3g}\"\"\".format(\n",
    "        *find_best_split_pt(X, Y, drop_in_gini)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e47c58",
   "metadata": {},
   "source": [
    "The following ranks the features according to the gains of their best binary splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_by_gini = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature,\n",
    "        **(lambda s, g: {\"split point\": s, \"gain\": g})(\n",
    "            *find_best_split_pt(df[feature], df.target, drop_in_gini)\n",
    "        ),\n",
    "    }\n",
    "    for feature in iris.feature_names\n",
    ").sort_values(by=\"gain\", ascending=False)\n",
    "rank_by_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc32f28",
   "metadata": {},
   "source": [
    "Using the entropy to measure impurity, we have the following alternative gain function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383866c",
   "metadata": {},
   "source": [
    "::::{prf:definition} \n",
    "\n",
    "The information gain is defined as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Gain_{\\R{A}}(D) &:= \\Info(D) - \\Info_{\\R{A}}(D) && \\text{where}\\\\\n",
    "\\Info_{\\R{A}}(D) &:= \\sum_{j} \\frac{|D_j|}{|D|} \\Info(D_j),\n",
    "\\end{align}\n",
    "$$ (Gain)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b236af",
   "metadata": {},
   "source": [
    "We will again consider the binary splitting criterion $\\R{X}\\leq s$ in particular, which gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Gain_{\\R{X}\\leq s}(D) = h(\\hat{P}_Y) - \\left[\\hat{P}[\\R{X}\\leq s] h(\\hat{p}_{\\R{Y}|\\R{X}\\leq s}) + \\hat{P}[\\R{X}> s]h(\\hat{p}_{\\R{Y}|\\R{X}> s})\\right]\n",
    "\\end{align}\n",
    "$$ (Gain-binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51b19c",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:gain\n",
    "\n",
    "Complete the following function to calculate the information gain on the target $\\R{Y}$ for a binary split $\\R{X}\\leq s$. You may use `dist` and `h` defined previously.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330e3e5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6b611522d3766568d847877cf846ec3",
     "grade": false,
     "grade_id": "info-gain",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def gain(X, Y, s):\n",
    "    \"\"\"Returns the information Gain of Y for the split X <= s.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: 1D array\n",
    "        Input feature values for different instances.\n",
    "    Y: 1D array\n",
    "        Target values corresponding to X.\n",
    "    s: Splitting point for X.\n",
    "    \"\"\"\n",
    "    S = X <= s\n",
    "    q = S.mean()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "print(f\"Information gain: {gain(X, Y, 0.8):.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd9b30c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3c3c5aaf0914408827891a35c41d00b",
     "grade": true,
     "grade_id": "test-info-gain",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "rank_by_entropy = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature,\n",
    "        **(lambda s, g: {\"split point\": s, \"gain\": g})(\n",
    "            *find_best_split_pt(df[feature], df.target, gain)\n",
    "        ),\n",
    "    }\n",
    "    for feature in iris.feature_names\n",
    ").sort_values(by=\"gain\", ascending=False)\n",
    "rank_by_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5494db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ab4b700bdec4cc39eb845394b3d641c",
     "grade": true,
     "grade_id": "htest-info-gain",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b4e17",
   "metadata": {},
   "source": [
    "The C4.5 induction algorithm uses information gain ratio instead of information gain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7dc22e",
   "metadata": {},
   "source": [
    "::::{prf:definition} \n",
    "\n",
    "The information gain ratio is defined as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\GainRatio_{\\R{A}}(D) &:= \\frac{\\Gain_{\\R{A}}(D)}{\\operatorname{SplitInfo}_{\\R{A}}(D)}\n",
    "\\end{align}\n",
    "$$ (GainRatio)\n",
    "\n",
    "which is normalized by \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\operatorname{SplitInfo}_{\\R{A}}(D) &:= h\\left(j\\mapsto \\frac{|D_j|}{|D|} \\right)=\\sum_j \\frac{|D_j|}{|D|}\\log \\frac{|D|}{|D_j|}.\n",
    "\\end{align}\n",
    "$$ (SplitInfo)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f2828a",
   "metadata": {},
   "source": [
    "For binary split $\\R{X}\\leq s$,\n",
    "\n",
    "$$\n",
    "\\operatorname{SplitInfo}_{\\R{X}\\leq s}(D) := h\\left(\\hat{P}[\\R{X}\\leq s], \\hat{P}[\\R{X}> s]\\right)\n",
    "$$ (SplitInfo-binary)\n",
    "\n",
    "in terms of the empirical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb802e",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:gain_ratio\n",
    "\n",
    "Complete the following function to calculate the *information gain ratio* for a binary split $\\R{X}\\leq s$ and target $\\R{Y}$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b964266",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6974ce63cdfadc7b512ba9e4ea330c7",
     "grade": false,
     "grade_id": "info-gain-ratio",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def gain_ratio(X, Y, split_pt):\n",
    "    S = X <= split_pt\n",
    "    q = S.mean()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02860cb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b1c1b36da376fcc022dfd4963ceff8f",
     "grade": true,
     "grade_id": "test-info-gain-ratio",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "rank_by_gain_ratio = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature,\n",
    "        **(lambda s, g: {\"split point\": s, \"gain\": g})(\n",
    "            *find_best_split_pt(df[feature], df.target, gain_ratio)\n",
    "        ),\n",
    "    }\n",
    "    for feature in iris.feature_names\n",
    ").sort_values(by=\"gain\", ascending=False)\n",
    "rank_by_gain_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788cb29",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cdd7f45cef622a317fd93f4d8db3c4c",
     "grade": true,
     "grade_id": "htest-info-gain-ratio",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa60da",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:difference\n",
    "\n",
    "Does the information gain ratio give a different ranking of the features? Why?\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302bdc2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d13962fbe953658626e1dc57921795e0",
     "grade": true,
     "grade_id": "difference",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
