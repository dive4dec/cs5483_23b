{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e54169",
   "metadata": {},
   "source": [
    "# Information Quantities for Probabilistic Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0979507",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4216011",
   "metadata": {},
   "source": [
    "This notebook will introduce the information quantities often used for training probabilistic classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c4750",
   "metadata": {},
   "source": [
    "As an example, the following handwritten digit classifier is [trained by deep learning](https://www.cs.cityu.edu.hk/~ccha23/deepbook/divedeep.html) using cross entropy loss:\n",
    "1. Handwrite a digit from 0, ..., 9.\n",
    "1. Click predict to see if the app can recognize the digit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0242c3",
   "metadata": {},
   "source": [
    "::::{card}\n",
    ":header: Open in [new tab](https://www.cs.cityu.edu.hk/~ccha23/mnist)\n",
    ":::{iframe} https://www.cs.cityu.edu.hk/~ccha23/mnist\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c015418",
   "metadata": {},
   "source": [
    "## Information Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef8a7c",
   "metadata": {},
   "source": [
    "A fundamental property of mutual information is that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302c6cb",
   "metadata": {},
   "source": [
    "::::{prf:theorem} Positivity of mutual information\n",
    ":label: MI-positivity\n",
    "\n",
    "$I(X;Y)\\geq 0$ with equality iff $X$ and $Y$ are independent.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90743a84",
   "metadata": {},
   "source": [
    "To show this, we think of the mutual information as a statistical distance called the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88b004",
   "metadata": {},
   "source": [
    "::::{prf:definition} \n",
    "\n",
    "The information/KL divergence between two probability measures $P$ and $Q$ over $\\mathcal{Z}$ is defined as\n",
    "\n",
    "$$\n",
    "D(P\\|Q) := \\int_{\\mathcal{Z}} (dP) \\log \\frac{dP}{dQ}.\n",
    "$$ (D)\n",
    "\n",
    "The conditional divergence is defined as \n",
    "\n",
    "$$\n",
    "D(V\\|W|U):=D(U\\times V\\| U\\times W).\n",
    "$$\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732607eb",
   "metadata": {},
   "source": [
    "For the information divergence to be called a [divergence](https://en.wikipedia.org/wiki/Divergence_(statistics)), it has to satisfy the following property:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f018b2",
   "metadata": {},
   "source": [
    "::::{prf:lemma} Positivity of divergence\n",
    ":label: D-positivity\n",
    "\n",
    "$D(P\\|Q) \\geq 0$, with equality iff $P=Q$ almost everywhere.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558af74",
   "metadata": {},
   "source": [
    "::::{prf:proof} \n",
    "\n",
    "Without loss of generality, we can rewrite the divergence as the expectation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D(P_{Z}\\|P_{Z'}) &:= E\\left[ \\frac{p_{Z}(Z')}{p_{Z'}(Z')} \\log\\frac{p_{Z}(Z')}{p_{Z'}(Z')} \\right]\\\\\n",
    "&\\geq E\\left[ \\frac{p_{Z}(Z')}{p_{Z'}(Z')}\\right] \\log \\underbrace{E\\left[\\frac{p_{Z}(Z')}{p_{Z'}(Z')} \\right]}_{=1} = 0,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the last inequality follows from [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) and the convexity of $r \\mapsto r \\log r$. Since $r$ is strictly convex, the inequality holds iff $\\frac{p_{Z}(Z')}{p_{Z'}(Z')}=1$ almost surely, i.e., $P_{Z}=P_{Z'}$ almost everywhere.  \n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9647f1e",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:1\n",
    "Prove the positivity of mutual information ({prf:ref}`MI-positivity`).\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30849b05",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56b7e14126c993c2956d0899146fd5df",
     "grade": true,
     "grade_id": "MI-positivity",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9300333",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cc7cb",
   "metadata": {},
   "source": [
    "A probabilistic classifier returns a conditional probability estimate $\\hat{P}_{Y|X}$ as a function of the training data $S$, which consists of i.i.d. samples of $(X,Y)$ but independent of $(X,Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb0ec3",
   "metadata": {},
   "source": [
    "A sensible choice of the loss function is\n",
    "\n",
    "$$\\ell(\\hat{P}_{Y|X}(\\cdot|x), P_{Y|X}(\\cdot|x)):=D(\\hat{P}_{Y|X}(\\cdot|x)\\|P_{Y|X}(\\cdot|x))$$ (D-loss)\n",
    "\n",
    "because, by the positivity of divergence (Lemma {prf:ref}`D-positivity`), the above loss is non-negative and equal to $0$ iff $P_X\\times \\hat{P}_{Y|X}=P_X \\times P_{Y|X}$ almost surely. Using this loss function, we have a simple bias-variance trade-off:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285ac29",
   "metadata": {},
   "source": [
    "::::{prf:theorem} Bias-variance trade-off\n",
    "\n",
    "The expected loss (risk) for the loss function {eq}`D-loss` is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[D(\\hat{P}_{Y|X} \\| P_{Y|X}|P_X)] = \\overbrace{I(S;\\hat{Y}|X)}^{\\text{Variance}} + \\overbrace{D(E[\\hat{P}_{Y|X}]\\|P_{Y|X}|P_X)}^{\\text{Bias}}\n",
    "\\end{align}\n",
    "$$ (Bias-Variance)\n",
    "\n",
    "where $\\hat{Y}$ is distributed according to\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P_{X,Y,S,\\hat{Y}}&=P_{X,Y}\\times P_{S} \\times P_{\\hat{Y}|X,S} && \\text{where}\\\\\n",
    "P_{\\hat{Y}|X,S}(y|x,s) &= \\hat{P}_{Y|X}(y|x) && \\text{for }(x,y,s)\\in \\mathcal{X}\\times \\mathcal{Y}\\times \\mathcal{S}.\n",
    "\\end{align}\n",
    "$$ (Yhat)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dac14f",
   "metadata": {},
   "source": [
    "- The variance $I(S;\\hat{Y}|X)$ (also $I(S;X,\\hat{Y})$ as $I(S;X)=0$) reflects the level of overfitting as it measures how much the estimate depends on the training data.\n",
    "- The bias $D(E[\\hat{P}_{Y|X}]\\|P_{Y|X}|P_X)$ reflects the level of underfitting as it measures how much the expected estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36718f",
   "metadata": {},
   "source": [
    "::::{prf:proof} \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[D(\\hat{P}_{Y|X} \\| P_{Y|X}|P_X)] \n",
    "&= E\\left[\\log \\frac{\\hat{p}_{Y|X}(\\hat{Y}|X)}{p_{Y|X}(\\hat{Y}|X)}\\right]\\\\\n",
    "&= \\underbrace{E\\left[\\log \\frac{\\hat{p}_{Y|X}(\\hat{Y}|X)}{E[\\hat{p}_{Y|X}](\\hat{Y}|X)}\\right]}_{\\text{(i)}} + \\underbrace{E\\left[\\log \\frac{E[\\hat{p}_{Y|X}](\\hat{Y}|X)]}{p_{Y|X}(\\hat{Y}|X)}\\right]}_{=D(E[\\hat{P}_{Y|X}]\\|P_{Y|X}|P_X) \\text{(bias)}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It remains to show (i) is the variance. By {eq}`Yhat`,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[\\hat{p}_{Y|X}](y|x)\n",
    "&= E[p_{\\hat{Y}|X,S}(y|x,S)|X=x]\\\\\n",
    "&= p_{\\hat{Y}|X}(y|x).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Substituting {eq}`Yhat` and the above into (i), we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{(i)} &= E\\left[\\log \\frac{p_{\\hat{Y}|X,S}(\\hat{Y}|X,S)}{p_{\\hat{Y}|X}(\\hat{Y}|X)}\\right]\\\\\n",
    "&= I(S;\\hat{Y}|X),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which completes the proof.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74be3e4",
   "metadata": {},
   "source": [
    "The loss in {eq}`D-loss`, however, cannot be evaluated on $S$ for training because $P_{Y|X}(\\cdot|x_i)$ is not known. Instead, we often use the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) loss\n",
    "\n",
    "$$\n",
    "\\ell(\\hat{P}_{Y|X}(\\cdot |x),y) := \\log \\frac{1}{\\hat{p}_{Y|X}(y|x)}.\n",
    "$$ (CE-loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded68cd",
   "metadata": {},
   "source": [
    "::::{prf:theorem} Cross entropy\n",
    "\n",
    "The risk for the loss in {eq}`CE-loss` is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E\\left[\\log \\frac1{\\hat{p}_{Y|X}(Y|X)}\\right] \n",
    "&= H(Y|X) + E[D(P_{Y|X}\\| \\hat{P}_{Y|X}|P_X)] \\\\\n",
    "&\\geq H(Y|X) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with equality iff $P_{X}\\times P_{Y|X}=P_{X}\\times \\hat{P}_{Y|X}$ almost everywhere.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bc673",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:2\n",
    "Prove the above result.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb944c48",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e23993eaaf0123173a83f5ba8585c90a",
     "grade": true,
     "grade_id": "CE-loss",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
