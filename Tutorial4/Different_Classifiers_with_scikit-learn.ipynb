{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5194a8",
   "metadata": {},
   "source": [
    "---\n",
    "title: Different Classifiers with scikit-learn\n",
    "math: \n",
    "    '\\abs': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\norm': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "    '\\mc': '\\mathcal{#1}'\n",
    "    '\\M': '\\boldsymbol{#1}'\n",
    "    '\\R': '\\mathsf{#1}'\n",
    "    '\\RM': '\\boldsymbol{\\mathsf{#1}}'\n",
    "    '\\op': '\\operatorname{#1}'\n",
    "    '\\E': '\\op{E}'\n",
    "    '\\d': '\\mathrm{\\mathstrut d}'\n",
    "    '\\Gini': '\\operatorname{Gini}'\n",
    "    '\\Info': '\\operatorname{Info}'\n",
    "    '\\Gain': '\\operatorname{Gain}'\n",
    "    '\\GainRatio': '\\operatorname{GainRatio}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8ff0a",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75731b",
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "from sklearn import datasets, neighbors, preprocessing, tree\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from util import plot_decision_regions\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b088af9",
   "metadata": {},
   "source": [
    "## Normalization of Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a3c88",
   "metadata": {},
   "source": [
    "For this notebook, we consider the binary classification problem on the [breast cancer dataset](https://doi.org/10.24432/C5DW2B) in [(Street el al. 2013)](https://doi.org/10.1117/12.148698):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bc5a1",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# load the dataset from sklearn\n",
    "dataset = datasets.load_breast_cancer()\n",
    "\n",
    "# create a DataFrame to help further analysis\n",
    "df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "df[\"target\"] = dataset.target\n",
    "df.target = df.target.astype(\"category\").cat.rename_categories(\n",
    "    dict(zip(range(3), dataset.target_names))\n",
    ")\n",
    "df  # display an overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ec66d",
   "metadata": {},
   "source": [
    "The goal is to train a classifier to diagnose whether a breast mass is malignant or benign. The target class distribution is shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "display(df.target.value_counts())\n",
    "df.target.value_counts().plot(kind=\"bar\", title=\"counts of different classes\", rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59f440c",
   "metadata": {},
   "source": [
    "The input features are characteristics of cell images obtained by [fine needle analysis (FNA)](https://en.wikipedia.org/wiki/Fine-needle_aspiration)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05b5ae",
   "metadata": {},
   "source": [
    "The following function displays the statistics of the features grouped by the class values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13225651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_statistics(df, **kwargs): \n",
    "    grps = df.groupby(\"target\", observed=False)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(grps), sharey=True, clear=True, figsize=(10, 9), layout=\"constrained\", squeeze=False, **kwargs)\n",
    "    for grp, ax in zip(df.groupby(\"target\", observed=False), axes[0]):\n",
    "        grp[1].boxplot(rot=90, fontsize=7, ax=ax).set_title(grp[0])\n",
    "    plt.show()\n",
    "\n",
    "show_feature_statistics(df, num=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941326f",
   "metadata": {},
   "source": [
    "From the above plots, it can be observed that the attributes `mean area` and `worst area` have much larger ranges than other features have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c2a04",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:1\n",
    "\n",
    " Is it true that a feature with a larger range is a better feature? Why?\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec04a05",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c4db5e8ba428be653c58f92819cf9c0",
     "grade": true,
     "grade_id": "range",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946df29",
   "metadata": {},
   "source": [
    "### Min-max Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a16112a",
   "metadata": {},
   "source": [
    "We can normalize a numeric feature $\\R{Z}$ to the unit interval as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\R{Z}':= \\frac{\\R{Z}}{b - a}\n",
    "\\end{align}\n",
    "$$ (min-max)\n",
    "\n",
    "where $a$ and $b$ are respectively the minimum and maximum possible values of $\\R{Z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63904677",
   "metadata": {},
   "source": [
    "$a$ and $b$ may be unknown in practice as the distribution of $\\R{Z}$ is unknown. We perform the normalization on the samples: The min-max normalization of the sequence (in $i$) of $z_i$ is the sequence of\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z'_i := \\frac{z_i - \\min_j z_j}{\\max_j z_j - \\min_j z_j},\n",
    "\\end{align}\n",
    "$$ (min-max-sample)\n",
    "\n",
    "where $\\min_j z_j$ and $\\max_j z_j$ are respectively the minimum and maximum sample values. It follows that $0\\leq z'_i \\leq 1$ and the equalities hold with equality for some indices $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9221a7",
   "metadata": {},
   "source": [
    "An implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normalize(df, suffix=\" (min-max normalized)\"):\n",
    "    \"\"\"Returns a new DataFrame with numerical attributes of the input DataFrame\n",
    "    min-max normalized.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input to be min-max normalized. May contain both numeric\n",
    "        and categorical attributes.\n",
    "    suffix: string\n",
    "        Suffix to append to the names of normalized attributes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame:\n",
    "        A copy of df with its numeric attributes replaced by their min-max\n",
    "        normalization. The normalized features are renamed with the suffix\n",
    "        appended to the end of their original names.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # avoid overwriting the original dataframe\n",
    "    min_values = df.select_dtypes(include=\"number\").min()  # Skip categorical features\n",
    "    max_values = df[min_values.index].max()\n",
    "\n",
    "    # min-max normalize\n",
    "    df[min_values.index] = (df[min_values.index] - min_values) / (\n",
    "        max_values - min_values\n",
    "    )\n",
    "\n",
    "    # rename normalized features\n",
    "    df.rename(columns={c: c + suffix for c in min_values.index}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbaeb75",
   "metadata": {},
   "source": [
    "It is a good idea to rename the normalized features to differentiate them from the original features. The following plots the statistics of the normalized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4792643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax_normalized = minmax_normalize(df)\n",
    "assert df_minmax_normalized.target.to_numpy().base is df.target.to_numpy().base\n",
    "\n",
    "show_feature_statistics(df_minmax_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084365f",
   "metadata": {},
   "source": [
    "After normalization, we can see how instances of different classes differ in different input features other than `mean area` and `worst area`. In particular, both `mean-concavity` and `worst-concavity` are substantially higher for malignant examples than for benign examples. Such details are hard to see in the plots before normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712d1e1",
   "metadata": {},
   "source": [
    "### Standard Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee6f48",
   "metadata": {},
   "source": [
    "Min-max normalization is not appropriate for features with unbounded support where $b-a=\\infty$ in {eq}`min-max`. The normalization factor $\\max_j z_j - \\min_j z_j$ in {eq}`min-max-sample` for i.i.d. samples will approach $\\infty$ as the number of samples goes to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a074c",
   "metadata": {},
   "source": [
    "Let us inspect the distribution of each feature using [`displot`](https://seaborn.pydata.org/generated/seaborn.displot.html) provided by the package [`seaborn`](https://seaborn.pydata.org), which was imported with\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    feature=dataset.feature_names, kernel_density_estimation=True, group_by_class=False\n",
    ")\n",
    "def plot_distribution(feature, kernel_density_estimation, group_by_class):\n",
    "    grps = df.groupby(\"target\", observed=False) if group_by_class else [('', df)]\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(grps), clear=True, figsize=(10, 5), layout=\"constrained\", num=4, squeeze=False, sharey=True)\n",
    "    for grp, ax in zip(grps, axes[0]):\n",
    "        sns.histplot(data=grp[1], x=feature, kde=kernel_density_estimation, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c152742",
   "metadata": {},
   "source": [
    "Play with the above widgets to check if the features appear to have unbounded support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5c24e",
   "metadata": {},
   "source": [
    "For a feature $\\R{Z}$ with unbounded support, one may use the $z$-score/standard normalization instead:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\R{Z}' := \\frac{\\R{Z} - E[\\R{Z}]}{\\sqrt{\\operatorname{Var}(\\R{Z})}}.\n",
    "\\end{align}\n",
    "$$ (standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bdf744",
   "metadata": {},
   "source": [
    "Since the distribution of $Z$ is unknown, we normalize the sequence of i.i.d. samples $z_i$ using its sample mean $\\mu$ and standard deviation $\\sigma$ to the sequence of\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z'_i := \\frac{z_i - \\mu}{\\sigma}. \n",
    "\\end{align}\n",
    "$$ (standard-sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e8d28",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:2\n",
    " Complete the function `standard_normalize` as follows:\n",
    "\n",
    "- Return a new copy of the input `DataFrame` `df` but with all its numeric attributes standard normalized. \n",
    "- You may use the methods `mean` and `std`.\n",
    "- Rename the normalized features by appending `suffix` to their names.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05049763",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67e1749ee14dbc231da0a6989707cc07",
     "grade": false,
     "grade_id": "normalize-attributes",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def standard_normalize(df, suffix=\" (standard normalized)\"):\n",
    "    \"\"\"Returns a DataFrame with numerical attributes of the input DataFrame\n",
    "    standard normalized.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input to be standard normalized. May contain both numeric\n",
    "        and categorical attributes.\n",
    "    suffix: string\n",
    "        Suffix to append to the names of normalized attributes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame:\n",
    "        A new copy of df that retains the categorical attributes but with the\n",
    "        numeric attributes replaced by their standard normalization.\n",
    "        The normalized features are renamed with the suffix appended to the end\n",
    "        of their original names.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "df_standard_normalized = standard_normalize(df)\n",
    "show_feature_statistics(df_standard_normalized, num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37908b8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04b496bd07b3c05177f5daf0ed39b658",
     "grade": true,
     "grade_id": "test-standard-normalization",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert np.isclose(\n",
    "    df_standard_normalized.select_dtypes(include=\"number\").mean(), 0\n",
    ").all()\n",
    "assert np.isclose(df_standard_normalized.select_dtypes(include=\"number\").std(), 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd3974a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daff4ae9be4ab1b656bb9d186062b798",
     "grade": true,
     "grade_id": "htest-standard-normalization",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb093321",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01700912",
   "metadata": {},
   "source": [
    "To create a $k$-nearest-neighbor ($k$-NN) classifier, we can use `sklearn.neighbors.KNeighborsClassifier`. The following fits a $1$-NN classifier to the entire dataset and returns its training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514de3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = df[dataset.feature_names], df.target\n",
    "kNN1 = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "kNN1.fit(X, Y)\n",
    "\n",
    "print(\"Training accuracy: {:0.3g}\".format(kNN1.score(X, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e95412",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:3\n",
    " Why is the training accuracy for $1$-NN $100\\%$? Explain according to how 1-NN works.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f5ff4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4af86ffab706c5d80300de98c370c2f",
     "grade": true,
     "grade_id": "1-NN",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3876277",
   "metadata": {},
   "source": [
    "To avoid overly-optimistic performance estimates, the following uses 10-fold cross validation to compute the accuracies of 1-NN trained on datasets with and without normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "dfs = {\"None\": df, \"Min-max\": df_minmax_normalized}\n",
    "\n",
    "acc = pd.DataFrame(columns=dfs.keys())\n",
    "for norm in dfs:\n",
    "    acc[norm] = cross_val_score(\n",
    "        kNN1,\n",
    "        dfs[norm].loc[:, lambda df: ~df.columns.isin([\"target\"])],\n",
    "        # not [dataset.feature_names] since normalized features are renamed\n",
    "        dfs[norm][\"target\"],\n",
    "        cv=cv,\n",
    "    )\n",
    "\n",
    "acc.agg([\"mean\", \"std\"]).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e082d900",
   "metadata": {},
   "source": [
    "The accuracies show that normalization improves the performance of 1-NN. More precisely, the accuracy improvement of $\\sim 5\\%$ appears statistically insignificant because it is at least twice the standard deviations of $\\sim 2\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf70bb5",
   "metadata": {},
   "source": [
    "::::{important}\n",
    "\n",
    "\n",
    "The proper way to compare performance should consider statistical significance, such as the [paired t-test](https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f). There is not much we can do to improve the statistical significance other than collecting more data. Repeating the cross-validation with different random seeds does not help as that only smooths out the randomness in splitting, not sampling. \n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b80d7",
   "metadata": {},
   "source": [
    "### Data Leak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39bee4",
   "metadata": {},
   "source": [
    "The accuracies computed for the normalizations above suffer from a subtle issue that renders them overly optimistic:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46e59b",
   "metadata": {},
   "source": [
    "::::{important}\n",
    "\n",
    "Since the normalization factors for cross validation were calculated from the entire dataset, the test data for each cross-validation fold may not be independent of the remaining normalized data for training the classifier. This subtle data leak may cause the performance estimate to be overly-optimistic.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69af98f",
   "metadata": {},
   "source": [
    "This issue can be resolved by computing the normalization factors from the training set instead of the entire dataset. To do so, we will create a pipeline using the following:\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "```\n",
    "\n",
    "- Like the filtered classifier in Weka, `sklearn.pipeline` provides the function `make_pipeline` to combine a filter with a classifier.\n",
    "- `sklearn.preprocessing` provides different filters for preprocessing features, , e.g., `StandardScaler` and `MinMaxScaler` for \n",
    "\n",
    "Creating a pipeline is especially useful for cross validation, where the normalization factors must be recomputed for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4112e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN1_standard_normalized = make_pipeline(preprocessing.StandardScaler(), kNN1)\n",
    "acc[\"Standard\"] = cross_val_score(kNN1_standard_normalized, X, Y, cv=cv)\n",
    "acc[\"Standard\"].agg([\"mean\", \"std\"]).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dfa893",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:4\n",
    " Similar to the above cell, correct the accuracies in `acc['Min-max']` to use `preprocessing.MinMaxScaler` as part of a pipeline for the 1-NN classifier.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68844c94",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe80c2cb5103bbe406d48e1c74c624df",
     "grade": false,
     "grade_id": "min-max",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "acc[\"Min-max\"].agg([\"mean\", \"std\"]).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f36428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02df299a4ac68648a291b58a2764dd51",
     "grade": true,
     "grade_id": "test-min-max",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf52917",
   "metadata": {},
   "source": [
    "### Decision Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00076d5",
   "metadata": {},
   "source": [
    "Since `sklearn` does not provide any function to plot the decision regions of a classifier, we provide the function `plot_decision_regions` in a module `util` defined in [`util.py`](util.py) of the current directory:\n",
    "\n",
    "```python\n",
    "from util import plot_decision_regions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beddb622",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "plot_decision_regions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b7038",
   "metadata": {},
   "source": [
    "The following plots the decision region for a selected pair of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Execute? [y/N]\").lower() == \"y\":\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, clear=True, figsize=(10, 10), layout=\"constrained\", num=6, sharey=True)\n",
    "    @interact(\n",
    "        normalization=[\"None\", \"Min-max\", \"Standard\"],\n",
    "        feature1=dataset.feature_names,\n",
    "        feature2=dataset.feature_names,\n",
    "        k=widgets.IntSlider(1, 1, 5, continuous_update=False),\n",
    "        resolution=widgets.IntSlider(1, 1, 4, continuous_update=False),\n",
    "    )\n",
    "    def decision_regions_kNN(\n",
    "        normalization,\n",
    "        feature1=dataset.feature_names[0],\n",
    "        feature2=dataset.feature_names[1],\n",
    "        k=1,\n",
    "        resolution=1,\n",
    "    ):\n",
    "        scaler = {\n",
    "            \"Min-max\": preprocessing.MinMaxScaler,\n",
    "            \"Standard\": preprocessing.StandardScaler,\n",
    "        }\n",
    "        kNN = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "        if normalization != \"None\":\n",
    "            kNN = make_pipeline(scaler[normalization](), kNN)\n",
    "        kNN.fit(df[[feature1, feature2]].to_numpy(), df.target.to_numpy())\n",
    "        ax.clear()\n",
    "        plot_decision_regions(\n",
    "            df[[feature1, feature2]], df.target, kNN, N=resolution * 100,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(\"Decision region for {}-NN\".format(k))\n",
    "        ax.set_xlabel(feature1)\n",
    "        ax.set_ylabel(feature2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1569f4b",
   "metadata": {},
   "source": [
    "Interact with the widgets to: \n",
    "\n",
    "- Learn the effect on the decision regions/boundaries with different normalizations and choices of $k$.\n",
    "- Learn to draw the decision boundaries for $1$-NN with min-max normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca5995",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:5\n",
    " Complete the following code to plot the decision regions for decision trees. Afterward, explain whether the decision regions change for different normalizations.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efceb47",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71d013b443a8a8820054eeb703f91dc5",
     "grade": false,
     "grade_id": "DT-region",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "if input(\"Execute? [y/N]\").lower() == \"y\":\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, clear=True, figsize=(10, 10), layout=\"constrained\", num=7, sharey=True)\n",
    "    @interact(\n",
    "        normalization=[\"None\", \"Min-max\", \"Standard\"],\n",
    "        feature1=dataset.feature_names,\n",
    "        feature2=dataset.feature_names,\n",
    "        resolution=widgets.IntSlider(1, 1, 4, continuous_update=False),\n",
    "    )\n",
    "    def decision_regions_kNN(\n",
    "        normalization,\n",
    "        feature1=dataset.feature_names[0],\n",
    "        feature2=dataset.feature_names[1],\n",
    "        resolution=1,\n",
    "    ):\n",
    "        scaler = {\n",
    "            \"Min-max\": preprocessing.MinMaxScaler,\n",
    "            \"Standard\": preprocessing.StandardScaler,\n",
    "        }\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ax.clear()\n",
    "        plot_decision_regions(\n",
    "            df[[feature1, feature2]], df.target, DT, N=resolution * 100,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(\"Decision region for Decision Tree\")\n",
    "        ax.set_xlabel(feature1)\n",
    "        ax.set_ylabel(feature2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85018c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f006f32a0176204e7bba0f6eb5a48ff3",
     "grade": true,
     "grade_id": "DT-decision-regions",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
